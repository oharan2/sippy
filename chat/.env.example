# LLM Configuration
# For local Ollama (default)
# I have found that llama3.1:8b and granite3.3:8b give the best results,
# with a slight edge to granite.
LLM_ENDPOINT=http://localhost:11434/v1
MODEL_NAME=granite3.3:8b

# For OpenAI
# LLM_ENDPOINT=https://api.openai.com/v1
# MODEL_NAME=gpt-4
# OPENAI_API_KEY=your_openai_api_key_here

# For Google Gemini
# MODEL_NAME=gemini-2.5-flash
# GOOGLE_API_KEY=your_google_api_key_here
# or GOOGLE_APPLICATION_CREDENTIALS=path_to_json_credentials

# Sippy API Configuration (for future use)
SIPPY_API_URL=https://sippy.dptools.openshift.org

# Jira Configuration (for known incident tracking)
JIRA_URL=https://issues.redhat.com

# Specify the MCP configuration to use.
MCP_CONFIG_FILE=mcp_config.json
