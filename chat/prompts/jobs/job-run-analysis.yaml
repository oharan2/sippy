# Comprehensive analysis of a CI job failure
name: job-run-analysis
description: Analyze a CI job run with detailed test failure patterns, log analysis, and root cause investigation
arguments:
  - name: job_url_or_id
    description: Prow job URL or numeric job ID (e.g., https://prow.ci.openshift.org/view/gs/test-platform-results/pr-logs/pull/openshift_cluster-network-operator/2433/pull-ci-openshift-cluster-network-operator-master-e2e-aws-ovn/1934795512955801600 or just 1934795512955801600)
    required: true
    type: string
prompt: |
  Perform a comprehensive failure analysis for the Prow job:
  {{ job_url_or_id }}

  Extract the numeric job ID from the URL if provided (look for a sequence of 16-19 digits), then analyze the failure systematically.

  ## Job Overview
  Use get_prow_job_summary to retrieve the job details, then report:
  - Job name and type (e.g., presubmit, periodic, postsubmit)
  - Job status and result (passed, failed, aborted)
  - When the job ran (format as human-readable date and time with "days/hours ago")
  - Direct links to:
    * Prow job page (primary link for viewing results)
    * Build logs and artifacts

  ## Aggregated Jobs Deep Dive (only if applicable)
  If the job is an aggregated job (combines results from multiple underlying job runs):
  - Identify that this is an aggregated job
  - Use the appropriate tools to retrieve underlying job runs that have failed tests
  - Analyze up to 5 of the underlying job runs with failures
  - For each underlying run with failures:
    * Get the job ID and status
    * Identify which tests failed in that specific run
    * Note any patterns in failure timing or distribution
  - Analyze patterns across the failed runs:
    * Are the same tests failing across multiple runs?
    * Are failures isolated to specific underlying runs?
    * Do failures correlate with specific configurations or infrastructure?

  ## Test Failure Analysis (only if applicable)
  If tests have failed, identify all failed tests from the job summary:
  - List each failed test with its status
  - For up to 5 failed tests, provide:
    * Test name and what it validates
    * Failure reason/error message (if available in summary)
    * Number of test runs and failure count
  - If more than 5 tests failed:
    * Report the total count
    * Note if this indicates a systemic issue (>10 failures often suggests cluster/infrastructure problems)
    * Group similar failures together if patterns emerge

  ## Failure Pattern Classification (only if applicable)
  If tests have failed, analyze the failure pattern:
  - **Single test failure**: Likely a specific test or feature issue
  - **Multiple related tests**: May indicate a component or subsystem problem
  - **Mass failures** (>10 tests): Often indicates:
    * Cluster setup/infrastructure failure
    * API server unavailability
    * Network connectivity issues
    * Storage subsystem failure

  ## Known Incidents Correlation
  Always check for known incidents:
  - Use check_known_incidents to see if there are ongoing Jira incidents
  - Compare incident descriptions, affected tests, and timing
  - If the failure matches a known incident:
    * Link to the incident
    * Note if this failure falls within the incident timeframe
    * Indicate whether the test failure is expected due to the incident

  ## Log Analysis
  If the initial analysis doesn't reveal clear root causes, perform log analysis. This is especially useful if no tests failed or if tests failed for unclear reasons.
  - Use analyze_job_logs with:
    * Default: path_glob="*build-log*", text_regex="[Ee]rror|[Ff]ail|panic|timeout"
    * For specific investigation: adjust patterns based on failure type
  - From log analysis, report:
    * Key error messages and stack traces
    * Timing of failures (early setup vs. test execution vs. teardown)
    * Infrastructure errors vs. application errors

  ## Root Cause Hypothesis
  Based on all collected evidence, provide:
  - **Most likely cause**: Specific hypothesis with supporting evidence
  - **Failure category**:
    * Product bug (feature not working as expected)
    * Test issue (flaky test, bad test code, environment assumption)
    * Infrastructure problem (cluster provisioning, network, storage)
    * Configuration issue (wrong flags, missing resources)
  - **Confidence level**: High/Medium/Low based on evidence clarity

  ## Context and Impact
  - If this is a presubmit job, it may be blocking a PR from merging
  - If this is a periodic/postsubmit job, it may indicate broader quality issues
  - Note if this appears to be an isolated failure or part of a pattern
  - Check if multiple similar jobs are failing (suggest checking related jobs if patterns emerge)

  ## Guidelines for Analysis
  - Be specific with evidence: cite exact error messages, test names, and timestamps
  - Use markdown formatting for readability
  - Include direct links to relevant resources (Prow, logs, Jira)
  - If you cannot determine something, say so explicitly rather than speculating
  - Focus on actionable insights that help the user understand and resolve the failure
  - If the job actually passed, note that clearly and explain what the user might have been looking for
  - NEVER report on TestGrid, do not provide TestGrid links
